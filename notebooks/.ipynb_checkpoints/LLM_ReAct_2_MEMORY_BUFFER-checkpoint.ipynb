{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "930d9192-f459-4d90-b219-73c8982cd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define libraries and packages\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "from typing import TypedDict, List, Optional,Annotated\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b7ff6a-af71-48ec-9914-b1cb80ab3601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Environment Variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe5b8a0-ab8c-4685-8447-bfac83a16662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amrutha sai\\AppData\\Local\\Temp\\ipykernel_6436\\3539105022.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n"
     ]
    }
   ],
   "source": [
    "# Define Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a93e8c-9ab6-4dfd-80b1-b03495501699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Identified:  local_docs\\Mahanati.pdf\n",
      "[Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-26T10:43:50+05:30', 'author': 'sai chitti', 'moddate': '2025-11-26T10:43:50+05:30', 'source': 'local_docs\\\\Mahanati.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content=\"MAHANATI.  \\n \\nMahanati is one of the best movies in Indian cinema which truly depicts the ups and downs of \\nSavithri amma's life which we all could connect to because we are also going through same \\nemotions. \\nIt was directed by Nag Ashwin and produced my Vyjyanthi, Swapna movies by 2 daring and dashing \\nsisters ‚Äì Swapna, Priyanka Dutt. Music done by Mickey J Meyer, till-date, remains his best work and \\none couldn‚Äôt even imagine that he did this movie. Just a soothing, heart-touching songs and \\nbackground score by this man. Just when you realize that music alone touches your heart and soul, \\nthere comes another man with his lyrics to make it even more worser ‚Äì Sirivennala Seetharama \\nSastry ‚Äì with Gelupuleni Samaram song. ‚Äúnaranaralona visham ayinidi prema‚Äù, SUMS IT ALL ‚Äì PEAK \\nCINEMA.   \\nOne couldn‚Äôt end this without mentioning about Keerthi suresh, she literally lived in Savithri amma \\nrole and no better than anyone could portray her, sorry ‚Äì no better than anyone could live Savithri \\namma‚Äôs life on-screen. She was also awarded the national award, which is so obvious for her \\nperformance. Credits to DQ too for accepting a role like Gemini Ganeshan and the love, romance, \\nchemistry between them was too nice ‚Äì ofc half of credits goes to nagi for his writing. Also Sai \\nMadhav Burra for his dialogues.  \\nOverall, It‚Äôs the BEST , BESTEST movie ever made in Telugu cinema atleast for me.\")]\n",
      "***************************\n",
      "File Identified:  local_docs\\SagaraSangamam.pdf\n",
      "[Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-26T10:44:34+05:30', 'author': 'sai chitti', 'moddate': '2025-11-26T10:44:34+05:30', 'source': 'local_docs\\\\SagaraSangamam.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='I also like SagaraSangamam by Vishwanath, which truly depicts of the story of a loser. Same as in \\nMahanti, this too depicts the ups-downs of life. But mahanati tops for me as favourite movie.')]\n",
      "***************************\n",
      "File Identified:  local_docs\\VaaranamAayiram.pdf\n",
      "[Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-26T10:46:05+05:30', 'author': 'sai chitti', 'moddate': '2025-11-26T10:46:05+05:30', 'source': 'local_docs\\\\VaaranamAayiram.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1'}, page_content='I think, Vaaranam Aayiram deserves the place here in third position. Again, its also slice of coming-\\nage movie depicting life.')]\n",
      "***************************\n"
     ]
    }
   ],
   "source": [
    "# Document Loader\n",
    "PDF_DIR = \"local_docs\"\n",
    "def load_all_pdfs(pdf_dir):\n",
    "    docs = []\n",
    "    for root, _, files in os.walk(pdf_dir):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(\".pdf\"):\n",
    "                path = os.path.join(root, f)\n",
    "                print(\"File Identified: \", path)\n",
    "                loader = PyPDFLoader(path)\n",
    "                pdf = loader.load()\n",
    "                print(pdf)\n",
    "                docs.extend(pdf)\n",
    "                print(\"***************************\")\n",
    "    return docs\n",
    "                \n",
    "docs = load_all_pdfs(PDF_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef911eb-593f-41b2-a4c4-c1bf7cfa8635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text splitter:  <langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x00000210E0222090>\n",
      "**************************************\n",
      "Final Docs: \n",
      "[Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-26T10:43:50+05:30', 'author': 'sai chitti', 'moddate': '2025-11-26T10:43:50+05:30', 'source': 'local_docs\\\\Mahanati.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'start_index': 0}, page_content=\"MAHANATI.  \\n \\nMahanati is one of the best movies in Indian cinema which truly depicts the ups and downs of \\nSavithri amma's life which we all could connect to because we are also going through same \\nemotions. \\nIt was directed by Nag Ashwin and produced my Vyjyanthi, Swapna movies by 2 daring and dashing \\nsisters ‚Äì Swapna, Priyanka Dutt. Music done by Mickey J Meyer, till-date, remains his best work and \\none couldn‚Äôt even imagine that he did this movie. Just a soothing, heart-touching songs and \\nbackground score by this man. Just when you realize that music alone touches your heart and soul, \\nthere comes another man with his lyrics to make it even more worser ‚Äì Sirivennala Seetharama \\nSastry ‚Äì with Gelupuleni Samaram song. ‚Äúnaranaralona visham ayinidi prema‚Äù, SUMS IT ALL ‚Äì PEAK \\nCINEMA.   \\nOne couldn‚Äôt end this without mentioning about Keerthi suresh, she literally lived in Savithri amma\"), Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-26T10:43:50+05:30', 'author': 'sai chitti', 'moddate': '2025-11-26T10:43:50+05:30', 'source': 'local_docs\\\\Mahanati.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'start_index': 789}, page_content='CINEMA.   \\nOne couldn‚Äôt end this without mentioning about Keerthi suresh, she literally lived in Savithri amma \\nrole and no better than anyone could portray her, sorry ‚Äì no better than anyone could live Savithri \\namma‚Äôs life on-screen. She was also awarded the national award, which is so obvious for her \\nperformance. Credits to DQ too for accepting a role like Gemini Ganeshan and the love, romance, \\nchemistry between them was too nice ‚Äì ofc half of credits goes to nagi for his writing. Also Sai \\nMadhav Burra for his dialogues.  \\nOverall, It‚Äôs the BEST , BESTEST movie ever made in Telugu cinema atleast for me.'), Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-26T10:44:34+05:30', 'author': 'sai chitti', 'moddate': '2025-11-26T10:44:34+05:30', 'source': 'local_docs\\\\SagaraSangamam.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'start_index': 0}, page_content='I also like SagaraSangamam by Vishwanath, which truly depicts of the story of a loser. Same as in \\nMahanti, this too depicts the ups-downs of life. But mahanati tops for me as favourite movie.'), Document(metadata={'producer': 'Microsoft¬Æ Word 2021', 'creator': 'Microsoft¬Æ Word 2021', 'creationdate': '2025-11-26T10:46:05+05:30', 'author': 'sai chitti', 'moddate': '2025-11-26T10:46:05+05:30', 'source': 'local_docs\\\\VaaranamAayiram.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'start_index': 0}, page_content='I think, Vaaranam Aayiram deserves the place here in third position. Again, its also slice of coming-\\nage movie depicting life.')]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    length_function\t= len\n",
    ")\n",
    "print(\"text splitter: \", text_splitter)\n",
    "print(\"**************************************\")\n",
    "chunks=text_splitter.split_documents(docs)\n",
    "print(\"Final Docs: \")\n",
    "print(chunks)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23b413fb-660e-4e11-b6dd-1a2357788f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store:  <langchain_community.vectorstores.chroma.Chroma object at 0x00000210E8C49E20>\n"
     ]
    }
   ],
   "source": [
    "chroma_dir = \"chromaDB\"\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=chroma_dir,\n",
    ")\n",
    "print(\"Vector store: \", vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9054a18a-d439-4d53-9b86-ace8201aaf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Chroma', 'HuggingFaceEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x00000210E8C49E20> search_kwargs={'k': 2}\n"
     ]
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 2}  \n",
    ")\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0e72891-b4a4-4946-939f-d4714437fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amrutha sai\\AppData\\Local\\Temp\\ipykernel_6436\\1881128354.py:3: LangChainDeprecationWarning: The class `GoogleSearchAPIWrapper` was deprecated in LangChain 0.0.33 and will be removed in 1.0. An updated version of the class exists in the `langchain-google-community package and should be used instead. To use it run `pip install -U `langchain-google-community` and import as `from `langchain_google_community import GoogleSearchAPIWrapper``.\n",
      "  google = GoogleSearchAPIWrapper()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Wrapper object:  search_engine=<googleapiclient.discovery.Resource object at 0x00000210EE078BF0> google_api_key='AIzaSyAtMANydAk2Z5OoVusAP7rtuawuj1ZQToo' google_cse_id='246e00ee0a15e4926' k=10 siterestrict=False\n",
      "[{'title': 'Mahesh Babu filmography - Wikipedia', 'link': 'https://en.wikipedia.org/wiki/Mahesh_Babu_filmography', 'snippet': 'It eventually grossed ‚Çπ212 crores worldwide against its budget of ‚Çπ200 crores, becoming a below-average grosser.. His next film titled Varanasi will be directed\\xa0...'}, {'title': 'Mahesh Babu (Prince) Movies | New and Upcoming Movies Of ...', 'link': 'https://www.filmibeat.com/celebs/mahesh-babu/upcoming-movies.html', 'snippet': 'Mahesh Babu Movies List: Find the latest updates and complete list of films of Mahesh Babu with their release date, movie ratings, and title only on\\xa0...'}, {'title': 'Rajamouli Mahesh babu upcoming film plot character prediction : r ...', 'link': 'https://www.reddit.com/r/tollywood/comments/ynze0r/rajamouli_mahesh_babu_upcoming_film_plot/', 'snippet': \"Nov 6, 2022 ... Basically Mission Impossible 4 with a more emotional touch, the hero will be loved for his patriotism and humanity. It won't be Tom Cruise type\\xa0...\"}, {'title': \"Mahesh Babu's first look from SS Rajamouli's movie revealed ...\", 'link': 'https://m.economictimes.com/magazines/panache/mahesh-babus-first-look-from-ss-rajamoulis-movie-revealed-check-plot-cast-and-expected-title/articleshow/123205987.cms', 'snippet': \"Aug 9, 2025 ... On Mahesh Babu's 50th birthday, SS Rajamouli unveiled the first teaser poster of their upcoming film, tagged #GlobeTrotter. The rugged close\\xa0...\"}, {'title': 'SS Rajamouli and Mahesh Babu next film in theaters 2028! : r ...', 'link': 'https://www.reddit.com/r/BollyBlindsNGossip/comments/1ezbjok/ss_rajamouli_and_mahesh_babu_next_film_in/', 'snippet': \"Aug 23, 2024 ... Rajamouli's next movie with Mahesh Babu is a jungle adventure set in the Africa. It is apparently based on Wilbur Smith's Ballantyne novels. r/\\xa0...\"}, {'title': \"SS Rajamouli and Mahesh Babu's upcoming film #SSMB29 after ...\", 'link': 'https://www.facebook.com/etimesofficial/posts/ss-rajamouli-and-mahesh-babus-upcoming-film-ssmb29-after-bringing-actors-priyank/1021214153521982/', 'snippet': 'Jun 8, 2025 ... S.S. Rajamouli is gearing up for his biggest project yet a jungle adventure tentatively titled SSMB29, starring Mahesh Babu in the lead, with\\xa0...'}, {'title': 'Rajamouli-Mahesh Babu next to based on quest for Sanjeevni Booti ...', 'link': 'https://www.reddit.com/r/tollywood/comments/1l74j8j/rajamoulimahesh_babu_next_to_based_on_quest_for/', 'snippet': 'Jun 9, 2025 ... Rajamouli-Mahesh Babu next to based on quest for Sanjeevni Booti. Likely to release in Summer 2027. NEWS.'}, {'title': 'Mahesh Babu (@urstrulymahesh) ‚Ä¢ Instagram photos and videos', 'link': 'https://www.instagram.com/urstrulymahesh/?hl=en', 'snippet': \"#NewTimesNewTRENDS #SSMB #NewTrends #MaheshBabuXTRENDS #StyledWithTRENDS \\u200b\\u200b [New ... Today let's rethink, and challenge our bias - Enjoy the film\\xa0...\"}, {'title': 'Future of Mahesh Babu : r/tollywood', 'link': 'https://www.reddit.com/r/tollywood/comments/13vs9m4/future_of_mahesh_babu/', 'snippet': \"May 30, 2023 ... Retirement from stardom - The 'young attractive look' is arguably Mahesh's biggest USP. After Rajamouli movie, that'll surely fade (I'm actually\\xa0...\"}, {'title': \"Varanasi trailer: Mahesh Babu's first look as Rudhra from SS ...\", 'link': 'https://www.hindustantimes.com/entertainment/telugu-cinema/varanasi-first-glimpse-mahesh-babus-first-look-from-ss-rajamouli-film-sees-him-wield-trishul-ride-bull-101763209394584.html', 'snippet': \"Nov 15, 2025 ... Mahesh Babu's upcoming film with SS Rajamouli has been titled\\xa0...\"}]\n"
     ]
    }
   ],
   "source": [
    "# Define GoogleSearch Wrapper\n",
    "\n",
    "google = GoogleSearchAPIWrapper()\n",
    "print(\"Google Wrapper object: \", google)\n",
    "\n",
    "# Test google search \n",
    "results = google.results(\"What is Mahesh Babu's upcoming film ?\", 10)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91f85fb5-c170-4244-8cda-7eafbec7f89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='models/gemini-2.0-flash' google_api_key=SecretStr('**********') client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000210EE07AB10> default_metadata=() model_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM model\n",
    "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ed6a7a8-e66f-4349-b1a2-da398669958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.State'>\n"
     ]
    }
   ],
   "source": [
    "# Define State of agent \n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    thoughts: List[str]\n",
    "    actions: List[str]\n",
    "    observations: List[str]\n",
    "    done: bool\n",
    "\n",
    "print(State)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a15726df-c372-4482-87ca-799599a8a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest Query\n",
    "\n",
    "def get_latest_query(state:State)->str:\n",
    "    \"\"\"\n",
    "    Robustly get latest human/user query from messages.\n",
    "    Supports both dict-style and LangChain message objects.\n",
    "    \"\"\"\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if msg.type == \"human\":     # HumanMessage\n",
    "            return msg.content\n",
    "        \n",
    "    return state[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14e01252-355a-4f80-8da4-df0a9ed83729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Tool Nodes\n",
    "\n",
    "def local_rag_node(state: State)->State:\n",
    "    \n",
    "    print(\"\\n[DEBUG] === LocalRAG Node ===\")\n",
    "    query = get_latest_query(state)\n",
    "    try:\n",
    "        result_docs = retriever.invoke(query)\n",
    "        if not result_docs:\n",
    "            observation = \"[LOCAL RAG] No results.\"\n",
    "        else:\n",
    "            observation = \"[LOCAL RAG]\\n\" + \"\\n\\n\".join(doc.page_content for doc in result_docs)\n",
    "    except Exception as e:\n",
    "        observation = f\"[LOCAL RAG ERROR] {e}\"\n",
    "\n",
    "    state[\"observations\"].append(observation)\n",
    "    state[\"messages\"] = add_messages(\n",
    "        state[\"messages\"], [{\"role\": \"assistant\", \"content\": observation}]\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def google_search_node(state: State)->State:\n",
    "    \n",
    "    print(\"\\n[DEBUG] === GoogleSearch Node ===\")\n",
    "    query = get_latest_query(state)\n",
    "\n",
    "    try:\n",
    "        results = google.results(query, num_results=5)\n",
    "        if not results:\n",
    "            observation = f\"[WEB ERROR] {e}\"\n",
    "        else:\n",
    "            lines = []\n",
    "            for r in results:\n",
    "                lines.append(\n",
    "                    f\"TITLE: {r.get('title')}\\n\"\n",
    "                    f\"SNIPPET: {r.get('snippet')}\\n\"\n",
    "                    f\"URL: {r.get('link')}\\n\"\n",
    "                    \"---\"\n",
    "                )\n",
    "            observation = \"[WEB SEARCH]\\n\" + \"\\n\".join(lines)\n",
    "    except Exception as e:\n",
    "        observation = f\"[WEB ERROR] {e}\"\n",
    "\n",
    "    state[\"observations\"].append(observation)\n",
    "    state[\"messages\"] = add_messages(\n",
    "        state[\"messages\"], [{\"role\": \"assistant\", \"content\": observation}]\n",
    "    )\n",
    "    return state\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e17035e7-508d-4d40-bfe2-7ba2e2a64019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REASON NODE\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an Agentic RAG assistant using the ReAct pattern.\n",
    "\n",
    "You have these TOOLS (actions):\n",
    "- local_rag      : query local PDF knowledge (vector DB)\n",
    "- google_search  : query the external web (Google Custom Search)\n",
    "- FINISH         : once you have enough information to answer\n",
    "\n",
    "Rules:\n",
    "- Use local_rag when query relates to user's personal info or stored PDF knowledge or organizational internal info specific to person/organization/enterprize interest.\n",
    "- Use web_search when external or recent info is needed, kind of factual and not personalized.\n",
    "- You may call multiple tools in sequence.\n",
    "- When you are ready to answer the user, use FINISH.\n",
    "\n",
    "You MUST always respond EXACTLY as:\n",
    "\n",
    "THOUGHT: <your internal reasoning>\n",
    "ACTION: <local_rag | google_search  | FINISH>\n",
    "\n",
    "Do NOT include anything else.\n",
    "\"\"\"\n",
    "\n",
    "def reason_node(state:State)->State:\n",
    "    print(\"\\n[DEBUG] === Reason Node ===\")\n",
    "    state.setdefault(\"thoughts\", [])\n",
    "    state.setdefault(\"actions\", [])\n",
    "    state.setdefault(\"observations\", [])\n",
    "    state.setdefault(\"done\", False)\n",
    "    history = \"\"\n",
    "    for t, a, o in zip(state[\"thoughts\"], state[\"actions\"], state[\"observations\"]):\n",
    "        history += f\"Thought: {t}\\nAction: {a}\\nObservation: {o}...\\n\\n\"\n",
    "\n",
    "    query = get_latest_query(state)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    {SYSTEM_PROMPT}\n",
    "    \n",
    "    ReAct History:\n",
    "    {history}\n",
    "\n",
    "    Conversation:\n",
    "    {state['messages']}\n",
    "\n",
    "    User Query:\n",
    "    {query}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt).content\n",
    "    print(f\"[DEBUG] LLM raw output in Reason:\\n{response}\\n\")\n",
    "\n",
    "    try:\n",
    "        thought = response.split(\"THOUGHT:\")[1].split(\"ACTION:\")[0].strip()\n",
    "        action = response.split(\"ACTION:\")[1].strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[EXCEPTION] {e}\\n\")\n",
    "        thought = response\n",
    "        action = \"FINISH\"\n",
    "\n",
    "    print(f\"[DEBUG] Parsed THOUGHT: {thought}\")\n",
    "    print(f\"[DEBUG] Parsed ACTION: {action}\")\n",
    "\n",
    "    state[\"thoughts\"].append(thought)\n",
    "    state[\"actions\"].append(action)\n",
    "\n",
    "    state[\"messages\"] = add_messages(\n",
    "        state[\"messages\"],\n",
    "        [{\"role\": \"assistant\", \"content\": f\"[THOUGHT] {thought} (ACTION={action})\"}]\n",
    "    )\n",
    "\n",
    "    if action == \"FINISH\":\n",
    "        state[\"done\"] = True\n",
    "\n",
    "    return state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33baa42f-c2cd-4ae2-8baa-affcf0182200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# FINAL GENERATE NODE\n",
    "# -----------------------\n",
    "\n",
    "def generate_node(state: State) -> State:\n",
    "    print(\"\\n[DEBUG] === FinalAnswer Node ===\")\n",
    "    state.setdefault(\"thoughts\", [])\n",
    "    state.setdefault(\"actions\", [])\n",
    "    state.setdefault(\"observations\", [])\n",
    "    state.setdefault(\"done\", False)\n",
    "    query = get_latest_query(state)\n",
    "    evidence = \"\\n\\n\".join(state[\"observations\"])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    User Query:\n",
    "    {query}\n",
    "    \n",
    "    Evidence from tools:\n",
    "    {evidence}\n",
    "    \n",
    "    Write a final answer to the user. Do NOT include the ReAct scratchpad or tool noise.\n",
    "    \"\"\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    print(f\"[DEBUG] Final synthesized answer:\\n{answer}\\n\")\n",
    "\n",
    "    state[\"messages\"] = add_messages(\n",
    "        state[\"messages\"], [{\"role\": \"assistant\", \"content\": answer}]\n",
    "    )\n",
    "    # üîÑ IMPORTANT: reset scratchpad so next user turn begins clean\n",
    "    state[\"thoughts\"] = []\n",
    "    state[\"actions\"] = []\n",
    "    state[\"observations\"] = []\n",
    "    state[\"done\"] = False\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e11460e-a616-432b-abfc-85842e3039d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langgraph.graph.state.StateGraph object at 0x00000210F81520C0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x210f81520c0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUILD LANGGRAPH\n",
    "builder = StateGraph(State)\n",
    "print(builder)\n",
    "\n",
    "builder.add_node(\"REASON\", reason_node)\n",
    "builder.add_node(\"LOCAL_RAG\", local_rag_node)\n",
    "builder.add_node(\"GOOGLE_SEARCH\", google_search_node)\n",
    "builder.add_node(\"GENERATE\", generate_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77b9a5c7-dd29-4db2-bf6d-ebfc4390ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Logic to decide which tool to call...\n",
    "\n",
    "def route_from_reason(state: State)->str:\n",
    "    print(f\"[DEBUG] Conditional Edge\")\n",
    "    action = state[\"actions\"][-1]\n",
    "    if action == \"local_rag\":\n",
    "        return \"LOCAL_RAG\"\n",
    "    elif action == \"google_search\":\n",
    "        return \"GOOGLE_SEARCH\"\n",
    "    elif action == \"FINISH\":\n",
    "        return \"GENERATE\"\n",
    "    return \"GENERATE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "951b8f0d-3c9a-4299-9356-a1a2a72104a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x210f81520c0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_edge(START, \"REASON\")\n",
    "builder.add_conditional_edges(\"REASON\", route_from_reason)\n",
    "builder.add_edge(\"LOCAL_RAG\", \"REASON\")\n",
    "builder.add_edge(\"GOOGLE_SEARCH\", \"REASON\")\n",
    "builder.add_edge(\"GENERATE\", END)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "299777bb-b0df-4367-a123-51abd73447b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory:  <langgraph.checkpoint.memory.InMemorySaver object at 0x00000210F826F560>\n"
     ]
    }
   ],
   "source": [
    "checkpointer = MemorySaver()\n",
    "print(\"Memory: \", checkpointer)\n",
    "graph = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bca337df-5cea-4a57-9b44-d344b7e36716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Agentic RAG ReAct (Local RAG + Web) Ready.\n",
      "Using thread_id = 'demo-chat'\n",
      "Type 'exit' or 'quit' to stop.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is inside my local PDFs?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] ===== Invoking graph for this turn =====\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: I should use the local_rag tool to find out what is inside the user's local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: I should use the local_rag tool to find out what is inside the user's local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The local PDFs contain information about the user's favorite movies, specifically \"Mahanati\" and \"SagaraSangamam\".\n",
      "ACTION: FINISH\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The local PDFs contain information about the user's favorite movies, specifically \"Mahanati\" and \"SagaraSangamam\".\n",
      "[DEBUG] Parsed ACTION: FINISH\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === FinalAnswer Node ===\n",
      "[DEBUG] Final synthesized answer:\n",
      "Based on the content of your local PDFs, you seem to have a preference for movies that depict the ups and downs of life. You specifically mention liking \"SagaraSangamam\" by Vishwanath, noting its portrayal of a loser's story. You also mention \"Mahanti,\" and while both movies depict similar themes, you consider \"Mahanti\" to be your favorite.\n",
      "\n",
      "[DEBUG] ===== Graph invocation finished =====\n",
      "\n",
      "Assistant: Based on the content of your local PDFs, you seem to have a preference for movies that depict the ups and downs of life. You specifically mention liking \"SagaraSangamam\" by Vishwanath, noting its portrayal of a loser's story. You also mention \"Mahanti,\" and while both movies depict similar themes, you consider \"Mahanti\" to be your favorite. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is my favorite movie? And why I do i like it so much ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] ===== Invoking graph for this turn =====\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking about their favorite movie and why they like it. This information is likely stored in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking about their favorite movie and why they like it. This information is likely stored in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking about their favorite movie and why they like it. This information is in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking about their favorite movie and why they like it. This information is in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking about their favorite movie and why they like it. This information is in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking about their favorite movie and why they like it. This information is in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking about their favorite movie and why they like it. This information is available in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking about their favorite movie and why they like it. This information is available in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "ACTION: local_rag\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: The user is asking a question that can be answered using the information in the local PDFs.\n",
      "[DEBUG] Parsed ACTION: local_rag\n",
      "[DEBUG] Conditional Edge\n",
      "\n",
      "[DEBUG] === LocalRAG Node ===\n",
      "\n",
      "[DEBUG] === Reason Node ===\n",
      "[DEBUG] LLM raw output in Reason:\n",
      "THOUGHT: Based on the information in your local PDFs, your favorite movie is \"Mahanti\". You like it because it depicts the ups and downs of life, similar to \"SagaraSangamam,\" which you also enjoy for its portrayal of a loser's story.\n",
      "ACTION: FINISH\n",
      "\n",
      "[DEBUG] Parsed THOUGHT: Based on the information in your local PDFs, your favorite movie is \"Mahanti\". You like it because it depicts the ups and downs of life, similar to \"SagaraSangamam,\" which you also enjoy for its portrayal of a loser's story.\n",
      "[DEBUG] Parsed ACTION: FINISH\n",
      "[DEBUG] Conditional Edge\n"
     ]
    },
    {
     "ename": "GraphRecursionError",
     "evalue": "Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGraphRecursionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     21\u001b[39m         final_reply = result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content\n\u001b[32m     22\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAssistant:\u001b[39m\u001b[33m\"\u001b[39m, final_reply, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mchat_in_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdemo-chat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mchat_in_loop\u001b[39m\u001b[34m(thread_id)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[DEBUG] ===== Invoking graph for this turn =====\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m result = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigurable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthread_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[DEBUG] ===== Graph invocation finished =====\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m final_reply = result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\main.py:3050\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3048\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3050\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3051\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3064\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\main.py:2661\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loop.status == \u001b[33m\"\u001b[39m\u001b[33mout_of_steps\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2653\u001b[39m     msg = create_error_message(\n\u001b[32m   2654\u001b[39m         message=(\n\u001b[32m   2655\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecursion limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mrecursion_limit\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reached \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2659\u001b[39m         error_code=ErrorCode.GRAPH_RECURSION_LIMIT,\n\u001b[32m   2660\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2661\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GraphRecursionError(msg)\n\u001b[32m   2662\u001b[39m \u001b[38;5;66;03m# set final channel values as run output\u001b[39;00m\n\u001b[32m   2663\u001b[39m run_manager.on_chain_end(loop.output)\n",
      "\u001b[31mGraphRecursionError\u001b[39m: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langgraph/errors/GRAPH_RECURSION_LIMIT"
     ]
    }
   ],
   "source": [
    "def chat_in_loop(thread_id: str = \"chat-thread-1\"):\n",
    "    \"\"\"\n",
    "    Multi-turn chat loop.\n",
    "    We never manually track messages ‚Äì MemorySaver does it for us.\n",
    "    \"\"\"\n",
    "    print(\"\\nüî• Agentic RAG ReAct (Local RAG + Web) Ready.\")\n",
    "    print(f\"Using thread_id = {thread_id!r}\")\n",
    "    print(\"Type 'exit' or 'quit' to stop.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in {\"exit\", \"quit\",\"stop\"}:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        print(\"\\n[DEBUG] ===== Invoking graph for this turn =====\")\n",
    "        result = graph.invoke(\n",
    "            {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n",
    "            config={\"configurable\": {\"thread_id\": thread_id}},\n",
    "        )\n",
    "        print(\"[DEBUG] ===== Graph invocation finished =====\\n\")\n",
    "\n",
    "        final_reply = result[\"messages\"][-1].content\n",
    "        print(\"Assistant:\", final_reply, \"\\n\")\n",
    "\n",
    "\n",
    "chat_in_loop(thread_id=\"demo-chat\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02c843-dceb-4834-a652-cafb5ac443f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
